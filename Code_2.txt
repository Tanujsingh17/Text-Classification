# Importing the necessary libraries
import pandas as pd
import numpy as np
import torch
import re
from torch.utils.data import TensorDataset, random_split
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from transformers import get_linear_schedule_with_warmup
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Loading the dataset
df = pd.read_csv("News_Data.csv")
categories = df["category"].unique()

# Preprocessing the text
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

def preprocess_text(text):
    text = text.lower() # Convert to lowercase
    text = re.sub(r'\d+', '', text) # Remove numbers
    text = re.sub(r'[^\w\s]','', text) # Remove punctuation
    text = text.strip() # Remove leading/trailing white space
    return text

df["text"] = df["text"].apply(preprocess_text)

# Splitting data into training and validation sets
train_size = 0.8
train_df = df.sample(frac=train_size, random_state=42)
val_df = df.drop(train_df.index).reset_index(drop=True)

# Converting text data into input features
train_encodings = tokenizer(list(train_df['text']), truncation=True, padding=True)
val_encodings = tokenizer(list(val_df['text']), truncation=True, padding=True)

train_labels = [categories.tolist().index(label) for label in train_df['category']]
val_labels = [categories.tolist().index(label) for label in val_df['category']]

train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']),
                              torch.tensor(train_encodings['attention_mask']),
                              torch.tensor(train_labels))

val_dataset = TensorDataset(torch.tensor(val_encodings['input_ids']),
                            torch.tensor(val_encodings['attention_mask']),
                            torch.tensor(val_labels))

# Defining the model architecture
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(categories))

# Setting up optimizer and the learning rate scheduler
optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)
epochs = 4
total_steps = len(train_dataset) * epochs
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps=0,
                                            num_training_steps=total_steps)

# Setting up data loaders
batch_size = 16
train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)
val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)

# Training the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

for epoch in range(epochs):
    model.train()
    total_train_loss = 0
    for batch in train_dataloader:
        batch = tuple(t.to(device) for t in batch)
        inputs = {
            'input_ids': batch[0],
            'attention_mask': batch[1],
            'labels': batch[2]
        }
        optimizer.zero_grad()
        outputs = model(**inputs)
        loss = outputs[0]
        total_train_loss += loss.item()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

    avg_train_loss = total_train_loss / len(train_dataloader)
    print("Epoch:", epoch + 1, "Train Loss:", avg_train_loss)

    model.eval()
    total_eval_accuracy = 0  # Initializing total evaluation accuracy
    total_eval_loss = 0  # Initializing total evaluation loss
    predictions, true_labels = [], []

    for batch in val_dataloader:
        batch = tuple(t.to(device) for t in batch)
        with torch.no_grad():
            inputs = {
                'input_ids': batch[0],
                'attention_mask': batch[1],
                'labels': batch[2]
            }
            outputs = model(**inputs)
            loss = outputs[0]
            total_eval_loss += loss.item()
            logits = outputs[1]
            logits = logits.detach().cpu().numpy()
            label_ids = inputs['labels'].cpu().numpy()
            predictions.append(np.argmax(logits, axis=1))
            true_labels.append(label_ids)

    total_eval_loss = total_eval_loss / len(val_dataloader)
    print("Epoch:", epoch + 1, "Validation Loss:", total_eval_loss)

    predictions = np.concatenate(predictions, axis=0)
    true_labels = np.concatenate(true_labels, axis=0)
    val_accuracy = accuracy_score(true_labels, predictions)
    total_eval_accuracy += val_accuracy

    print("Epoch:", epoch + 1, "Validation Accuracy:", val_accuracy)
    print("Average Validation Accuracy:",
          total_eval_accuracy / (epoch + 1))

