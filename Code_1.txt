# Importing the necessary libraries
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from nltk.corpus import stopwords

# Loading the pre-trained BERT tokenizer and the model
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)

# Loading the news articles dataset
df = pd.read_csv('News_Data.csv')

# Preprocessing the text data
df['text'] = df['text'].apply(lambda x: x.lower())
df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords.words('english'))]))
df['text'] = df['text'].str.replace('[^\w\s]','')

# Splitting the data into training and test sets
train_df, test_df = train_test_split(df, test_size=0.2)

# Converting the data to PyTorch tensors
train_texts = train_df['text'].tolist()
train_labels = train_df['category'].tolist()
train_encodings = tokenizer(train_texts, truncation=True, padding=True)
train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']),
                                               torch.tensor(train_encodings['attention_mask']),
                                               torch.tensor(train_labels))

test_texts = test_df['text'].tolist()
test_labels = test_df['category'].tolist()
test_encodings = tokenizer(test_texts, truncation=True, padding=True)
test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']),
                                              torch.tensor(test_encodings['attention_mask']),
                                              torch.tensor(test_labels))

# Defining the training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy='steps',
    eval_steps=500,
    load_best_model_at_end=True
)

# Defining the trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=lambda pred: {'accuracy': accuracy_score(pred.label_ids, pred.predictions.argmax(-1)),
                                  'precision': precision_score(pred.label_ids, pred.predictions.argmax(-1), average='weighted'),
                                  'recall': recall_score(pred.label_ids, pred.predictions.argmax(-1), average='weighted'),
                                  'f1': f1_score(pred.label_ids, pred.predictions.argmax(-1), average='weighted')}
)

# Fine-tuning the model
trainer.train()

# Evaluating the model on the test set
trainer.evaluate()

# Making predictions on a few samples from the test set
test_sample = ['Apple unveils new products at their event.',
               'Messi scores a hat-trick in the match against Real Madrid.',
               'NASA announces the discovery of a new exoplanet.',
               'Donald Trump elected as the new President of the United States.']
test_sample_encodings = tokenizer(test_sample, truncation=True, padding=True)
test_sample_dataset = torch.utils.data.TensorDataset(torch.tensor(test_sample_encodings['input_ids']),
                                                     torch.tensor(test_sample_encodings['attention_mask']))

# Using the trained model to make predictions
predictions = trainer.predict(test_sample_dataset)
predicted_labels = np.argmax(predictions.predictions

# Printing the predicted labels for the test samples
print(predicted_labels)
